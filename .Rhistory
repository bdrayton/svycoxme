my_samp <- pop[pop$id %in% samp_cluster_ids, ]
my_samp$prob <- (specs$n_clusters_in_samp/specs$n_clusters)
my_samp$weights <- my_samp$prob^-1
# rescale_weights
my_samp$rweights <- (1/my_samp$prob)/mean(1/my_samp$prob)
my_samp <- my_samp[order(my_samp$stat_time), ]
# the regular fit.
coxme_fit <- coxme::coxme(survival::Surv(stat_time, stat)~ X1 + X2 + (1|id), data = my_samp, weights = rweights)
# calculate ui and Hessian
parts <- make_parts(coxme_fit, my_samp, weights = my_samp$weights)
hessian <- calc_Di(parts)
ui <- calc_ui(parts)
# define design, add ui
my_des <- svydesign(~id, weights = ~weights, data = my_samp)
my_des_rep <- as.svrepdesign(my_des)
uvars <- paste("u", colnames(ui), sep = "_")
my_des$variables[uvars] <- as.matrix(ui)
svytotal_form1 <- formula(paste("~", paste(uvars, collapse = " + ")))
Nhat <- sum(weights(my_des))
svycoxme_fit <- svycoxme(survival::Surv(stat_time, stat) ~ X1 + X2 + (1 | id), des = sv)
# calculate sandwich variance
# division by Nhat needed?
meat_ui <- eval(bquote(vcov(svytotal(.(svytotal_form1), design = my_des))/Nhat))
my_bread <- solve(-hessian)
sandwich <- (my_bread %*% meat_ui %*% my_bread)/Nhat
list(
specs = specs,
sandwich_var  = diag(sandwich)[1:2],
svycoxme_var = diag(vcov(svycoxme_fit)),
coefs = coef(svycoxme_fit))
}
try_one_rep <- function(...){
stream <- .Random.seed
r <- try(one_rep(...))
attr(r, "stream") <- stream
r
}
# set to 20 for Ihaka
cores <- 5
# set to 20 for Ihaka
cores <- 5
cl <- parallel::makeCluster(cores)
# put objects in place that might be needed for the code
parallel::clusterExport(cl, c("one_rep", "try_one_rep", "svycoxme_path"))
# Set a different seed on each member of the cluster (just in case)
# this could be set to avoid errors. currently one error.
parallel::clusterSetRNGStream(cl, iseed = 3528942)
parallel::clusterEvalQ(cl, {
# devtools::load_all(path = "/home/bdra011/svycoxme")
devtools::load_all(svycoxme_path)
# devtools::load_all("C:/Users/Bradley/Documents/PhD_local/svycoxme")
})
run_one_pop <- function(theta){
# Build pop
pop_list <- lapply(cluster_str_list, function(cluster_info){
k <- cluster_info$Freq
nk <- as.numeric(as.character(cluster_info$Size))
k_id <- formatC(k, width = max_cluster_freq_digits, flag = "0")
nk_id <- formatC(nk, width = max_cluster_digits, flag = "0")
the_data <- one_dataset(~X1 + X2 + (1 | M1),
dists = list(X1 = ~rnorm(n),
X2 = ~rnorm(n),
M1 = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = c(X1 = 1, X2 = 1),
random_effect_variance = list(M1 = theta)
)
dplyr::mutate(the_data, id = paste(nk_id,k_id, M1, sep = "_" ))
})
pop <- Reduce(rbind.data.frame, pop_list)
parallel::clusterExport(cl, c("pop"), envir = environment())
fits <- parallel::parLapply(cl, param_combos_list, try_one_rep)
fits
}
test_fits <- run_one_pop(1)
test_fits
# define one rep
one_rep <- function(specs){
# one cluster sample
samp_cluster_ids <- unique(pop$id)[sample.int(specs$n_clusters, specs$n_clusters_in_samp)]
my_samp <- pop[pop$id %in% samp_cluster_ids, ]
my_samp$prob <- (specs$n_clusters_in_samp/specs$n_clusters)
my_samp$weights <- my_samp$prob^-1
# rescale_weights
my_samp$rweights <- (1/my_samp$prob)/mean(1/my_samp$prob)
my_samp <- my_samp[order(my_samp$stat_time), ]
# the regular fit.
coxme_fit <- coxme::coxme(survival::Surv(stat_time, stat)~ X1 + X2 + (1|id), data = my_samp, weights = rweights)
# calculate ui and Hessian
parts <- make_parts(coxme_fit, my_samp, weights = my_samp$weights)
hessian <- calc_Di(parts)
ui <- calc_ui(parts)
# define design, add ui
my_des <- svydesign(~id, weights = ~weights, data = my_samp)
my_des_rep <- as.svrepdesign(my_des)
uvars <- paste("u", colnames(ui), sep = "_")
my_des$variables[uvars] <- as.matrix(ui)
svytotal_form1 <- formula(paste("~", paste(uvars, collapse = " + ")))
Nhat <- sum(weights(my_des))
svycoxme_fit <- svycoxme(survival::Surv(stat_time, stat) ~ X1 + X2 + (1 | id), des = my_des_rep)
# calculate sandwich variance
# division by Nhat needed?
meat_ui <- eval(bquote(vcov(svytotal(.(svytotal_form1), design = my_des))/Nhat))
my_bread <- solve(-hessian)
sandwich <- (my_bread %*% meat_ui %*% my_bread)/Nhat
list(
specs = specs,
sandwich_var  = diag(sandwich)[1:2],
svycoxme_var = diag(vcov(svycoxme_fit)),
coefs = coef(svycoxme_fit))
}
test_fits <- run_one_pop(1)
table(sapply(test_fits, inherits, "try-error"))
test_fits[[1]]
# define one rep
one_rep <- function(specs){
# one cluster sample
samp_cluster_ids <- unique(pop$id)[sample.int(specs$n_clusters, specs$n_clusters_in_samp)]
my_samp <- pop[pop$id %in% samp_cluster_ids, ]
my_samp$prob <- (specs$n_clusters_in_samp/specs$n_clusters)
my_samp$weights <- my_samp$prob^-1
# rescale_weights
my_samp$rweights <- (1/my_samp$prob)/mean(1/my_samp$prob)
my_samp <- my_samp[order(my_samp$stat_time), ]
# the regular fit.
coxme_fit <- coxme::coxme(survival::Surv(stat_time, stat)~ X1 + X2 + (1|id), data = my_samp, weights = rweights)
# calculate ui and Hessian
parts <- make_parts(coxme_fit, my_samp, weights = my_samp$weights)
hessian <- calc_Di(parts)
ui <- calc_ui(parts)
# define design, add ui
my_des <- svydesign(~id, weights = ~weights, data = my_samp)
my_des_rep <- as.svrepdesign(my_des)
uvars <- paste("u", colnames(ui), sep = "_")
my_des$variables[uvars] <- as.matrix(ui)
svytotal_form1 <- formula(paste("~", paste(uvars, collapse = " + ")))
Nhat <- sum(weights(my_des))
svycoxme_fit <- svycoxme(survival::Surv(stat_time, stat) ~ X1 + X2 + (1 | id), des = my_des_rep)
# calculate sandwich variance
# division by Nhat needed?
meat_ui <- eval(bquote(vcov(svytotal(.(svytotal_form1), design = my_des))/Nhat))
my_bread <- solve(-hessian)
sandwich <- (my_bread %*% meat_ui %*% my_bread)/Nhat
list(
specs = specs,
sandwich_var  = diag(sandwich)[1:2],
svycoxme_var = diag(vcov(svycoxme_fit)),
coefs = coef(svycoxme_fit))
}
try_one_rep <- function(...){
stream <- .Random.seed
r <- try(one_rep(...))
attr(r, "stream") <- stream
r
}
# put objects in place that might be needed for the code
parallel::clusterExport(cl, c("one_rep", "try_one_rep", "svycoxme_path"))
run_one_pop <- function(theta){
# Build pop
pop_list <- lapply(cluster_str_list, function(cluster_info){
k <- cluster_info$Freq
nk <- as.numeric(as.character(cluster_info$Size))
k_id <- formatC(k, width = max_cluster_freq_digits, flag = "0")
nk_id <- formatC(nk, width = max_cluster_digits, flag = "0")
the_data <- one_dataset(~X1 + X2 + (1 | M1),
dists = list(X1 = ~rnorm(n),
X2 = ~rnorm(n),
M1 = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = c(X1 = 1, X2 = 1),
random_effect_variance = list(M1 = theta)
)
dplyr::mutate(the_data, id = paste(nk_id,k_id, M1, sep = "_" ))
})
pop <- Reduce(rbind.data.frame, pop_list)
parallel::clusterExport(cl, c("pop"), envir = environment())
fits <- parallel::parLapply(cl, param_combos_list, try_one_rep)
fits
}
test_fits <- run_one_pop(1)
table(sapply(test_fits, inherits, "try-error"))
test_fits
all_fits <- lapply(thetas, run_one_pop)
#stop the cluster
parallel::stopCluster(cl)
is.error <- sapply(fits, inherits, "try-error")
is.error <- sapply(all_fits, inherits, "try-error")
# none
table(is.error)
all_fits_unlist <- unlist(all_fits, recursive = FALSE)
is.error <- sapply(all_fits_unlist, inherits, "try-error")
# none
table(is.error)
all_fits_unlist <- unlist(all_fits, recursive = FALSE)
is.error <- sapply(all_fits_unlist, inherits, "try-error")
# none
table(is.error)
names(all_fits_unlist[[1]])
shape_res <- function(one_fit){
covars <- names(one_fit$coefs)
v1 <- diag(one_fit$sandwich_var)
v2 <- diag(one_fit$svycoxme_var)
data.frame(
method = rep(c("sandwich", "bootstrap"), each = 2),
covariate = rep(covars, 2),
coefs = rep(one_fit$coefs, 2),
variance = c(v1, v2),
lower = one_fit$coefs + qnorm(0.025) * sqrt(c(v1, v2)),
upper = one_fit$coefs + qnorm(0.975) * sqrt(c(v1, v2))
)
}
shape_res(fits[[1]])
shape_res(all_fits_unlist[[1]])
shape_res <- function(one_fit){
covars <- names(one_fit$coefs)
v1 <- one_fit$sandwich_var
v2 <- one_fit$svycoxme_var
data.frame(
method = rep(c("sandwich", "bootstrap"), each = 2),
covariate = rep(covars, 2),
coefs = rep(one_fit$coefs, 2),
variance = c(v1, v2),
lower = one_fit$coefs + qnorm(0.025) * sqrt(c(v1, v2)),
upper = one_fit$coefs + qnorm(0.975) * sqrt(c(v1, v2))
)
}
shape_res(all_fits_unlist[[1]])
fits_df <- lapply(fits[!is.error], shape_res)
fits_df <- lapply(all_fits_unlist[!is.error], shape_res)
df <- Reduce(rbind.data.frame, fits_df)
library(tidyverse)
df %>%
ggplot(aes(sqrt(variance))) +
facet_grid(rows = vars(covariate, method), scales = 'free') +
geom_density()
bias <- function(theta, true_theta){
sum(theta - true_theta)/length(theta)
}
df %>%
group_by(covariate, method) %>%
summarise(bias = bias(coefs, true_theta = 1))
EmpSE <- function(theta){
theta_bar = mean(theta)
sqrt( sum((theta-mean(theta))^2)/(length(theta)-1) )
}
df %>%
group_by(method, covariate) %>%
summarise(bias = bias(coefs, true_theta = 1),
EmpSE = EmpSE(coefs),
MeanSE = mean(sqrt(variance)),
diff = EmpSE - MeanSE)
# number of 'hits'
# about the same coverage. Will need to check for a range of parameters and sampling situations.
df %>%
group_by(method, covariate) %>%
mutate(hit = lower < 1 & upper > 1) %>%
summarise(mean(hit))
df %>%
arrange(lower) %>%
group_by(covariate) %>%
mutate(hit = lower < 1 & upper > 1,
index = row_number()) %>%
ggplot(aes(y = index, xmin = lower, xmax = upper, colour = hit)) +
geom_errorbarh(height = 0) +
facet_grid(rows = vars(covariate)) +
geom_vline(xintercept = 1)
devtools::load_all(".")
devtools::load_all(".")
Sys.info()
Sys.getenv()
svycoxme::svycoxme
devtools::load_all(".")
svycoxme::svycoxme
svycoxme
svycoxme.svyrep.design
devtools::load_all(".")
cluster_str <- data.frame(table(Size = rpois(2500, 2) + 6)) |>
dplyr::filter(Freq >=10)
cluster_str_list <- split(cluster_str, seq(nrow(cluster_str)))
max_cluster_digits <- max(nchar(as.character(cluster_str$Size)))
max_cluster_freq_digits <- max(nchar(as.character(cluster_str$Freq)))
set.seed(949742)
pop_list <- lapply(cluster_str_list, function(cluster_info){
k <- cluster_info$Freq
nk <- as.numeric(as.character(cluster_info$Size))
k_id <- formatC(k, width = max_cluster_freq_digits, flag = "0")
nk_id <- formatC(nk, width = max_cluster_digits, flag = "0")
the_data <- one_dataset(~X1 + (1 | M1),
dists = list(X1 = ~rnorm(n),
M1 = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = c(1),
random_effect_variance = list(M1 = 0)
)
dplyr::mutate(the_data, id = paste(nk_id,k_id, M1, sep = "_" ))
})
devtools::load_all(".")
cluster_str <- data.frame(table(Size = rpois(2500, 2) + 6)) |>
dplyr::filter(Freq >=10)
cluster_str_list <- split(cluster_str, seq(nrow(cluster_str)))
max_cluster_digits <- max(nchar(as.character(cluster_str$Size)))
max_cluster_freq_digits <- max(nchar(as.character(cluster_str$Freq)))
set.seed(949742)
pop_list <- lapply(cluster_str_list, function(cluster_info){
k <- cluster_info$Freq
nk <- as.numeric(as.character(cluster_info$Size))
k_id <- formatC(k, width = max_cluster_freq_digits, flag = "0")
nk_id <- formatC(nk, width = max_cluster_digits, flag = "0")
the_data <- one_dataset(~X1 + (1 | M1),
dists = list(X1 = ~rnorm(n),
M1 = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = c(1),
random_effect_variance = list(M1 = 0)
)
dplyr::mutate(the_data, id = paste(nk_id,k_id, M1, sep = "_" ))
})
pop <- Reduce(rbind.data.frame, pop_list)
# simple random sample
my_samp <- pop[sample.int(nrow(pop), 500),]
# sort samp by time.
my_samp <- my_samp[order(my_samp$stat_time),]
my_samp$weight = 1
N_hat <- sum(my_samp$weight)
BB <- function(formula, weight = NULL, data, beta){
model_frame <- model.frame(formula, data = data)
# first column (response) is actually a (masked) matrix with time and status.
response <- as.matrix(model_frame[ ,1])
time <- response[,"time"]
stat <- response[,"status"]
# sort data by time
time_order <- order(time)
time <- time[time_order]
stat <- stat[time_order]
weight <- weight[time_order]
model_frame <- model_frame[time_order, ]
model_matrix <- model.matrix(formula, data = model_frame)
# calculate the weighted risk sets.
# get X terms
X <- model_matrix[,-1, drop = FALSE]
# this should fail if beta and X don't match dimensions
# I need these to be Matrix::Matrix
risk_score <- Matrix::Matrix(X %*% beta)
# weighted.
exp_risk_score <- weight * exp(risk_score)
rev_exp_risk_score <- exp_risk_score
rev_exp_risk_score@x <- rev(exp_risk_score@x)
# this is S0_hat in binder
at_risk <- Matrix::Matrix(rev(cumsum(rev_exp_risk_score)), ncol = 1)
# this is S1_hat
exp_risk_score_X <- exp_risk_score * X
at_risk_X <- fast_risk_sets(exp_risk_score_X)
at_risk_X_X <- fast_risk_sets(exp_risk_score_X * X)
list(stat = stat,
time = time,
weight = weight,
S0 = at_risk,
S1 = at_risk_X,
exp_risk_score = exp(risk_score),
weighted_exp_risk_score = exp_risk_score,
S2 = at_risk_X_X,
X = X)
}
coxfit <- survival::coxph(survival::Surv(stat_time, stat) ~ X1, data = my_samp)
parts <- BB(survival::Surv(stat_time, stat) ~ X1, data = my_samp,
beta = coef(coxfit), weight = my_samp$weight)
calc_ui2 <- function(parts){
n <- length(parts$stat)
# first term is the same
term1 <- with(parts, {
stat * (X - S1/S0)
})
# divide each exp(beta*X) by the series of risk sets.
p1 <- with(parts, {
tcrossprod(exp_risk_score, 1/S0)
})
p2 <- with(parts, {
# replicate X n times to make a n*n matrix from n*1 X.
# minus S0/S1, which is also n*1 from each row
Matrix(as.numeric(X), ncol = n, nrow = n) - Matrix(as.numeric(S1/S0), ncol = n, nrow = n, byrow = TRUE)
})
# yi is the indicator I(ti >= tj). Each ti needs to be compared to all tj
# because ti is sorted, this should be an upper triangle matrix (if no ties)
yi <- with(parts, {
Matrix(rep(time, each = n) <= rep(time, n), ncol = n, nrow = n, byrow = TRUE)
})
# matrix with n reps of stat, as rows.
dj <- with(parts, {
Matrix(stat, nrow = n, ncol = n, byrow = TRUE)
})
# matrix with n reps of weights, as rows.
wj <- with(parts, {
Matrix(weight, nrow = n, ncol = n, byrow = TRUE)
})
# the second term is the row sums after taking the product of these matricies
# term2 <- rowSums(-dj * wj * yi * p1 * p2)
term2 <- -dj * wj * yi * p1 * p2
# term1 + term2
# to compare to the ui matrix, filled cell by cell.
term2
}
r2 <- calc_ui2(parts)
svycoxme_path <- file.path(Sys.getenv("OneDriveCommercial"), "Documents/PhD_local/svycoxme")
devtools::load_all(svycoxme_path)
k <- 5000
nk <- 10
n_clusters_in_sample <- 40
theta <- 0
true_coefs = c(1, -0.7, 0.5)
the_data <- one_dataset(~X1 + X2 + X3 + (1 | M),
dists = list(X1 = ~rnorm(n),
X2 = ~rep(rnorm(k), each = nk),
X3 = ~rep(rbinom(k, 1, 0.5), each = nk),
M = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = true_coefs,
random_effect_variance = c(M=theta)
)
pop <- dplyr::mutate(the_data, id = M)
devtools::load_all(".")
k <- 5000
nk <- 10
n_clusters_in_sample <- 40
cluster_str <- data.frame(table(Size = rpois(2500, 2) + 6)) |>
dplyr::filter(Freq >=10)
cluster_str_list <- split(cluster_str, seq(nrow(cluster_str)))
max_cluster_digits <- max(nchar(as.character(cluster_str$Size)))
max_cluster_freq_digits <- max(nchar(as.character(cluster_str$Freq)))
set.seed(949742)
pop_list <- lapply(cluster_str_list, function(cluster_info){
k <- cluster_info$Freq
nk <- as.numeric(as.character(cluster_info$Size))
k_id <- formatC(k, width = max_cluster_freq_digits, flag = "0")
nk_id <- formatC(nk, width = max_cluster_digits, flag = "0")
the_data <- one_dataset(~X1 + (1 | M1),
dists = list(X1 = ~rnorm(n),
M1 = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = c(1),
random_effect_variance = list(M1 = 0)
)
dplyr::mutate(the_data, id = paste(nk_id,k_id, M1, sep = "_" ))
})
pop <- Reduce(rbind.data.frame, pop_list)
names(parts)
length(parts$stat)
i = 250
X[i]
parts$X[i]
svycoxme_path <- file.path(Sys.getenv("OneDriveCommercial"), "Documents/PhD_local/svycoxme")
devtools::load_all(svycoxme_path)
k <- 5000
nk <- 10
n_clusters_in_sample <- 40
theta <- 0
true_coefs = c(1, -0.7, 0.5)
the_data <- one_dataset(~X1 + X2 + X3 + (1 | M),
dists = list(X1 = ~rnorm(n),
X2 = ~rep(rnorm(k), each = nk),
X3 = ~rep(rbinom(k, 1, 0.5), each = nk),
M = ~rep(1:k, each = nk),
error = ~rexp(n, 10),
stat = ~sample(rep(c(0, 1), round(n * c(0.2, 0.8))), n)),
dist_args = list(k = k, nk = nk,
n = k * nk),
coefficients = true_coefs,
random_effect_variance = c(M=theta)
)
pop <- dplyr::mutate(the_data, id = M)
# sample from it
# one cluster sample
samp_cluster_ids <- unique(pop$id)[sample.int(k, n_clusters_in_sample)]
my_samp <- pop[pop$id %in% samp_cluster_ids, ]
## for memory reasons
rm(pop)
rm(the_data)
my_samp$prob <- (n_clusters_in_sample/k)
my_samp$weights <- my_samp$prob^-1
# rescale_weights
my_samp$rweights <- (1/my_samp$prob)/mean(1/my_samp$prob)
my_samp <- my_samp[order(my_samp$stat_time), ]
coxph_fit <- survival::coxph(survival::Surv(stat_time, stat)~ X1 + X2 + X3,
data = my_samp, weights = my_samp$rweights)
parts <- make_parts(coxph_fit, my_samp, weights = my_samp$weights)
parts
names(parts)
i = 250
parts$X[i]
coxph_fit <- survival::coxph(survival::Surv(stat_time, stat)~ X1 + X2 + X3,
data = my_samp, weights = my_samp$rweights)
parts <- make_parts(coxph_fit, my_samp, weights = my_samp$weights)
names(parts)
parts$X
parts$X[i, ]
